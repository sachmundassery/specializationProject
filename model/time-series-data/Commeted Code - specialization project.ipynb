{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# The idea behind StandardScaler is that it will transform your data such that its distribution \n",
    "# will have a mean value 0 and standard deviation of 1\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "from scipy.stats import boxcox\n",
    "# SciPy's stats package provides a function called boxcox for performing box-cox power transformation \n",
    "# that takes in original non-normal data as input and returns fitted data along with the lambda value \n",
    "# that was used to fit the non-normal distribution to normal distribution.\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "import statsmodels.api as sm\n",
    "#statsmodels is a Python module that provides classes and functions for the estimation of many different \n",
    "# statistical models, as well as for conducting statistical tests, and statistical data exploration.\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "# Augmented Dickey Fuller test (ADF Test) is a common statistical test used to test whether a given \n",
    "# Time series is stationary or not. The adfuller function returns a tuple of statistics from the ADF \n",
    "# test such as the Test Statistic, P-Value, Number of Lags Used, Number of Observations used for the \n",
    "# ADF regression and a dictionary of Critical Values.\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "# KPSS test is a statistical test to check for stationarity of a series around a deterministic trend. \n",
    "# Like ADF test, the KPSS test is also commonly used to analyse the stationarity of a series.\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "# A correlogram (also called Auto Correlation Function ACF Plot or Autocorrelation plot) is a visual way \n",
    "# to show serial correlation in data that changes over time (i.e. time series data). Serial correlation \n",
    "# (also called autocorrelation) is where an error at one point in time travels to a subsequent point in time.\n",
    "# A plot of the autocorrelation of a time series by lag is called the AutoCorrelation Function, or the acronym ACF. \n",
    "# This plot is sometimes called a correlogram or an autocorrelation plot.Running the example creates a 2D plot \n",
    "# showing the lag value along the x-axis and the correlation on the y-axis between -1 and 1.\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "from statsmodels.graphics.tsaplots import plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching the required csv files\n",
    "stores = pd.read_csv(\"../input/rossman-store-capstone/store.csv\")\n",
    "train = pd.read_csv(\"../input/rossman-store-capstone/train.csv\", parse_dates=True, index_col=\"Date\", low_memory = False)\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# to get the dimensions of the dataset\n",
    "print(\"Stores dataset: {}\".format(stores.shape))\n",
    "print(\"Train dataset: {}\".format(train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores.info()\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nunique() function return number of unique elements in the object. \n",
    "# Store is key column and is unique for all the records. Also we have a total of 1115 stores in total\n",
    "\n",
    "stores[\"Store\"].nunique()\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# As expected in the train file, we have the time series data for the 1115 stores in 'stores' dataset\n",
    "\n",
    "train['Store'].nunique()\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# Checking the index of train dataset\n",
    "# it will display the index on which the rows are identified \n",
    "train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the need to merge 2 data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging 'stores' and 'train' dataset based on the key 'Stores'\n",
    "\n",
    "data = pd.merge(train, stores, how=\"inner\", on=\"Store\", validate=\"m:1\")\n",
    "\n",
    "# paramters : \n",
    "# 1. Name of the 1st data frame object\n",
    "# 2. Name of the 2nd data frame object\n",
    "# 3. inner join : Use intersection of keys\n",
    "# 4. on âˆ’ Columns (names) to join on. Must be found in both the left and right DataFrame objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merged dataset shape: {}\".format(data.shape))\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['StoreType', 'Assortment', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "for col in cols:\n",
    "    print(data.loc[data['Store']==1,col].value_counts(dropna=False))\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# loc[rows,columns]\n",
    "# rows : selecting rows with 1 in 'store' column   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the stores we are selecting for our study\n",
    "store_ids = [1, 3, 8, 9, 13, 25, 29, 31, 46]\n",
    "\n",
    "# now we are re-assigning our dataframe with the informations of the required stores\n",
    "train = train.loc[train['Store'].isin(store_ids)]\n",
    "stores = stores.loc[stores['Store'].isin(store_ids)]\n",
    "\n",
    "# isin(): Pandas isin() method is used to filter data frames. \n",
    "# isin() method helps in selecting rows with having a particular(or Multiple) value in a particular column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing 'store' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of stores under consideration\n",
    "\n",
    "stores[\"Store\"].nunique()\n",
    "# ie. we are considering only 9 unique stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting some specific columns of stores dataset\n",
    "cols = ['StoreType', 'Assortment', 'Promo2']\n",
    "\n",
    "for col in cols:\n",
    "    print(stores[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of stores that has not run any promo: {}\".format(stores.loc[stores['Promo2']==0,'Store'].count()))\n",
    "print(\"Stores that hasn't run any promo: \")\n",
    "print(stores.loc[stores['Promo2']==0,'Store'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the promo fields are NaN for stores that has 'Promo2' as 0\n",
    "print(stores.loc[stores['Promo2']==0,'Promo2SinceWeek'].value_counts(dropna=False))\n",
    "print(stores.loc[stores['Promo2']==0, 'Promo2SinceYear'].value_counts(dropna=False))\n",
    "print(stores.loc[stores['Promo2']==0, 'PromoInterval'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing 'train' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if we have the data for all the dates for each of the store\n",
    "\n",
    "print(train.groupby('Store').size().value_counts())\n",
    "\n",
    "# We have data for 942 dates for 934 stores, while for 180 stores we only have data for 758 dates \n",
    "# and for 1 store we have data for 941 dates\n",
    "stores_942 = train.groupby('Store').size()[train.groupby('Store').size()==942].index.to_list()\n",
    "print(stores_942)\n",
    "\n",
    "stores_758 = train.groupby('Store').size()[train.groupby('Store').size()==758].index.to_list()\n",
    "print(stores_758)\n",
    "\n",
    "# stores_941 = train.groupby('Store').size()[train.groupby('Store').size()==941].index.to_list()\n",
    "# print(len(stores_941))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converting the date field in to a datetime object\n",
    "\n",
    "# train.loc[:,'Date'] = pd.to_datetime(train.loc[:,'Date'], format='%Y-%m-%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
